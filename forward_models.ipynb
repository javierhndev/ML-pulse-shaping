{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cc9ad9d",
   "metadata": {},
   "source": [
    "# Forward models\n",
    "\n",
    "This notebook reproduces the resutls from the forward models presented in the paper.\n",
    "\n",
    "It needs two datasets:\n",
    "- The reconstructed pulse shapes (need to use `pulse_reconstruction_hr.ipynb`)\n",
    "- The experimental dataset from galadriel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df02851",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import models.fcnn as models_fcnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d6a115",
   "metadata": {},
   "source": [
    "### LOADING THE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599a3a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename='../laps-ml/datasets/galadriel_dataset_24_09_18_high_res.h5'\n",
    "filename_pulse='../laps-ml/datasets/pulse_240918.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a755b570",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the Dazller input data\n",
    "df_input=pd.read_hdf(filename,'df_input')\n",
    "\n",
    "#read the reconstructed pulse data\n",
    "df_time_200=pd.read_hdf(filename_pulse,'df_time_200')\n",
    "df_pulse_200=pd.read_hdf(filename_pulse,'df_pulse_200')\n",
    "t_200=df_time_200.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a482d25b",
   "metadata": {},
   "source": [
    "### Drop shots with bad goodness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36efe138",
   "metadata": {},
   "outputs": [],
   "source": [
    "goodness_val=1.3\n",
    "df_input_reduced=df_input[df_input['goodness']>goodness_val]\n",
    "\n",
    "good_shots_list=df_input_reduced.index.values\n",
    "#print(good_shots_list)\n",
    "df_pulse_200_reduced=df_pulse_200.iloc[good_shots_list]\n",
    "\n",
    "#reset the index\n",
    "df_input_reduced.reset_index(inplace=True,drop=True)\n",
    "df_pulse_200_reduced.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36924f8d",
   "metadata": {},
   "source": [
    "### Define the model variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbb69a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the dataset (X:dazzler param, Y:wizzler)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_input_reduced, df_pulse_200_reduced, test_size=0.3, random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684a417b",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17179b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are using: cuda\n"
     ]
    }
   ],
   "source": [
    "# set the device we will be using to train the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device=\"cpu\"\n",
    "print(\"We are using:\",device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ec98d4",
   "metadata": {},
   "source": [
    "### Linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2606a437",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lin_forward(X_train, X_test, y_train, y_test):\n",
    "    #Reset the index on y_test to have the same indexes as y_predict\n",
    "    y_test_reset=y_test.reset_index(drop=True)\n",
    "\n",
    "    forward_model_lin=LinearRegression().fit(X_train,y_train)\n",
    "\n",
    "    y_predict_forward=forward_model_lin.predict(X_test)\n",
    "    \n",
    "    #study the error distribution\n",
    "    df_error_forward_lin=abs(y_test_reset-y_predict_forward)\n",
    "    df_error_forward_lin=df_error_forward_lin.sum(axis=1)/y_test.shape[1] #sum error / num columns\n",
    "    \n",
    "    \n",
    "    return forward_model_lin, df_error_forward_lin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4365a3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_model_lin,df_error_forward_lin=train_lin_forward(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80894e33",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84a510a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rf_forward(X_train, X_test, y_train, y_test):\n",
    "    #Reset the index on y_test to have the same indexes as y_predict\n",
    "    y_test_reset=y_test.reset_index(drop=True)\n",
    "\n",
    "    #train forward model with Random forest\n",
    "    n_estimators = 300\n",
    "    max_features = 1.0#'sqrt'\n",
    "    max_depth=20\n",
    "    random_state=18\n",
    "\n",
    "    forward_model_rf=RandomForestRegressor(n_estimators=n_estimators,\n",
    "                                            max_features=max_features,\n",
    "                                            max_depth=max_depth,\n",
    "                                              random_state=random_state)\n",
    "    forward_model_rf.fit(X_train, y_train)\n",
    "    y_predict_forward=forward_model_rf.predict(X_test)\n",
    "\n",
    "    #study the erro distribution\n",
    "    df_error_forward_rf=abs(y_test_reset-y_predict_forward)\n",
    "    df_error_forward_rf=df_error_forward_rf.sum(axis=1)/y_test.shape[1] #sum error / num columns\n",
    "\n",
    "    return forward_model_rf,df_error_forward_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36a98ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_model_rf,df_error_forward_rf=train_rf_forward(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51e3769",
   "metadata": {},
   "source": [
    "### Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95dd82b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AcceleratorError",
     "evalue": "CUDA error: CUDA-capable device(s) is/are busy or unavailable\nSearch for `cudaErrorDevicesUnavailable' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAcceleratorError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m forward_model_mlp=models_fcnn.FWmodelNN(X_train,y_train,X_test,y_test,device)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mforward_model_mlp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;66;43;03m#epochs \u001b[39;49;00m\n\u001b[32m      3\u001b[39m \u001b[43m                 \u001b[49m\u001b[32;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;66;43;03m#batch size\u001b[39;49;00m\n\u001b[32m      4\u001b[39m \u001b[43m                 \u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#print freq\u001b[39;49;00m\n\u001b[32m      5\u001b[39m \u001b[43m                 \u001b[49m\u001b[32;43m0.001\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#learning rate\u001b[39;00m\n\u001b[32m      6\u001b[39m y_predict_nn_fwd=forward_model_mlp.predict(X_test)\n\u001b[32m      7\u001b[39m error_fwd_model_mlp=forward_model_mlp.error_calc_mae()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SDSC/projects/fusion_GA/galadriel/paper_ml_pulse_shaping/models/fcnn.py:67\u001b[39m, in \u001b[36mFWmodelNN.train\u001b[39m\u001b[34m(self, n_epochs, batch_size, print_freq, learning_rate)\u001b[39m\n\u001b[32m     64\u001b[39m         model=perceptron_fwd(\u001b[38;5;28mself\u001b[39m.in_features,\u001b[38;5;28mself\u001b[39m.out_features)\n\u001b[32m     66\u001b[39m         \u001b[38;5;66;03m#training\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m         \u001b[38;5;28mself\u001b[39m.nn_fc_model, \u001b[38;5;28mself\u001b[39m.train_error, \u001b[38;5;28mself\u001b[39m.test_error=\u001b[43mtrain_nn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43mX_test_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_test_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m\t\t\t\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43mprint_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SDSC/projects/fusion_GA/galadriel/paper_ml_pulse_shaping/models/fcnn.py:173\u001b[39m, in \u001b[36mtrain_nn\u001b[39m\u001b[34m(X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor, model, device, n_epochs, batch_size, print_freq, learning_rate)\u001b[39m\n\u001b[32m    171\u001b[39m startTime = time.time()\n\u001b[32m    172\u001b[39m \u001b[38;5;66;03m#Create the model and define the loss and optimizer\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m173\u001b[39m nn_fc_model=\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[38;5;66;03m#nn_model=cnn().to(device)\u001b[39;00m\n\u001b[32m    175\u001b[39m \u001b[38;5;66;03m#print(nn_fc_model)\u001b[39;00m\n\u001b[32m    177\u001b[39m loss_func=nn.MSELoss() \u001b[38;5;66;03m#mean squared error\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv_galadriel/lib/python3.12/site-packages/torch/nn/modules/module.py:1371\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1368\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1369\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1371\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv_galadriel/lib/python3.12/site-packages/torch/nn/modules/module.py:930\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    929\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m    933\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    934\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    935\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    940\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    941\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv_galadriel/lib/python3.12/site-packages/torch/nn/modules/module.py:957\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    953\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    954\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    955\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    956\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m957\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    958\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    960\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_subclasses\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfake_tensor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FakeTensor\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv_galadriel/lib/python3.12/site-packages/torch/nn/modules/module.py:1357\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1350\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t.dim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m):\n\u001b[32m   1351\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1352\u001b[39m             device,\n\u001b[32m   1353\u001b[39m             dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1354\u001b[39m             non_blocking,\n\u001b[32m   1355\u001b[39m             memory_format=convert_to_format,\n\u001b[32m   1356\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1357\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1358\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1359\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1360\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1361\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1362\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1363\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mAcceleratorError\u001b[39m: CUDA error: CUDA-capable device(s) is/are busy or unavailable\nSearch for `cudaErrorDevicesUnavailable' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "forward_model_mlp=models_fcnn.FWmodelNN(X_train,y_train,X_test,y_test,device)\n",
    "forward_model_mlp.train(100,#epochs \n",
    "                 128,#batch size\n",
    "                 20, #print freq\n",
    "                 0.001) #learning rate\n",
    "y_predict_nn_fwd=forward_model_mlp.predict(X_test)\n",
    "error_fwd_model_mlp=forward_model_mlp.error_calc_mae()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae1da43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_galadriel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
